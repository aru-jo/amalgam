{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds the Methane network.\n",
    "===================================\n",
    "Implements the _inference/loss/training pattern_ for model building.\n",
    "1. inference() - Builds the model as far as is required for running the network forward to make predictions.\n",
    "2. loss() - Adds to the inference model the layers required to generate loss.\n",
    "3. training() - Adds to the loss model the Ops required to generate and apply gradients.\n",
    "\n",
    "This file is used by the various \"fully_connected_*.py\" files and not meant to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import  patch_size\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = patch_size.patch_size\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE *192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model up to where it may be used for inference.\n",
    "--------------------------------------------------\n",
    "Args:\n",
    "* images: Images placeholder, from inputs().\n",
    "* hidden1_units: Size of the first hidden layer.\n",
    "* hidden2_units: Size of the second hidden layer.\n",
    "\n",
    "Returns:\n",
    "* softmax_linear: Output tensor with the computed logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images, conv1_channels, conv2_channels, fc1_units, fc2_units):\n",
    "    \n",
    "    # Conv 1\n",
    "    with tf.name_scope('conv_1') as scope:\n",
    "        weights = tf.get_variable('weights', shape=[5, 5, 192, conv1_channels], \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        biases = tf.get_variable('biases', shape=[conv1_channels], initializer=tf.constant_initializer(0.05))\n",
    "        \n",
    "        # converting the 1D array into a 3D image\n",
    "        x_image = tf.reshape(images, [-1,IMAGE_SIZE,IMAGE_SIZE,192])\n",
    "        z = tf.nn.conv2d(x_image, weights, strides=[1, 1, 1, 1], padding='VALID')\n",
    "        h_conv1 = tf.nn.relu(z+biases, name=scope.name)\n",
    "    \n",
    "    # Maxpool 1\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME', name='h_pool1')\n",
    "    \n",
    "    # Conv2\n",
    "    with tf.variable_scope('h_conv2') as scope:\n",
    "        weights = tf.get_variable('weights', shape=[5, 5, conv1_channels, conv2_channels], \n",
    "                                  initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        biases = tf.get_variable('biases', shape=[conv2_channels], initializer=tf.constant_initializer(0.05))\n",
    "        z = tf.nn.conv2d(h_pool1, weights, strides=[1, 1, 1, 1], padding='VALID')\n",
    "        h_conv2 = tf.nn.relu(z+biases, name=scope.name)\n",
    "    \n",
    "    # Maxpool 2\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME', name='h_pool2')\n",
    "   \n",
    "    # FIXED in python file\n",
    "    #size_after_conv_and_pool_twice = 4\n",
    "    size_after_conv_and_pool_twice = int(math.ceil((math.ceil(float(IMAGE_SIZE-KERNEL_SIZE+1)/2)-KERNEL_SIZE+1)/2))\n",
    "    \n",
    "    #Reshape from 4D to 2D\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, (size_after_conv_and_pool_twice**2)*conv2_channels])\n",
    "    \n",
    "    # FC 1\n",
    "    with tf.name_scope('h_FC1') as scope:\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([size_after_conv_and_pool_twice, fc1_units],\n",
    "                                stddev=1.0 / math.sqrt(float(size_after_conv_and_pool_twice))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([fc1_units]),\n",
    "                             name='biases')\n",
    "        h_FC1 = tf.nn.relu(tf.matmul(h_pool2_flat, weights) + biases, name=scope.name)\n",
    "        \n",
    "    # FC 2\n",
    "    with tf.name_scope('h_FC2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([fc1_units, fc2_units],\n",
    "                                stddev=1.0 / math.sqrt(float(fc1_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([fc2_units]),\n",
    "                             name='biases')\n",
    "        h_FC2 = tf.nn.relu(tf.matmul(h_FC1, weights) + biases, name=scope.name)\n",
    "    \n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([fc2_units, NUM_CLASSES],\n",
    "                                stddev=1.0 / math.sqrt(float(fc2_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(h_FC2, weights) + biases\n",
    "    \n",
    "    \n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits, labels, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Training OP\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss, learning_rate):\n",
    "    tf.scalar_summary(loss.op.name, loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Evaluation OP\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
